{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验：搭建三层神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验概要\n",
    "\n",
    "掌握了 NumPy 多维数组的运算，就可以高效地实现神经网络。首先，复习一下 Numpy 数组的知识点。\n",
    "\n",
    "#### 多维数组\n",
    "\n",
    "简单地讲，多维数组就是“数字的集合”，数字排成一列的集合、排成长方形的集合、排成三维状或者（更加一般化的）N 维状的集合都称为：**多维数组**。Python 的索引从 **`0`** 开始，因此，第一个维度对应第 0 维，第二个维度对应第 1 维，以此类推。二维数组也称为：**矩阵（matrix）**。数组的横向排列称为 **行（row）**，纵向排列称为 **列（column）**。\n",
    "\n",
    "下面我们创建一个具有两个平面的三维数组 (**Three dimensional (3D) array**)：\n",
    "\n",
    "![](./img/1_04.png)\n",
    "\n",
    "其中，数组的维数可以通过 `np.ndim()` 函数获得，数组的形状可以通过实例变量 `shape` 获得，如下代码所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 2]\n",
      "  [3 4]]\n",
      "\n",
      " [[5 6]\n",
      "  [7 8]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # 导入numpy科学计算库\n",
    "\n",
    "# 创建一个三维数组\n",
    "A = np.array([[[1,2],[3,4]],\n",
    "              [[5,6],[7,8]]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出数组维数，输出 \"3\"\n",
    "np.ndim(A) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出数组形状，输出\"(2,2,2)\"\n",
    "A.shape   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`shape` 输出的形状 `(2, 2, 2)` 分别代表三维数组的：平面数，行数，列数 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 矩阵乘积 Matrix Multiplication 与点积函数 np.dot ()\n",
    "\n",
    "下面，复习矩阵（二维数组）的乘积。矩阵相乘的标准方法，不是将一个矩阵中的每个元素与另一个矩阵的每个元素相乘（这称为：按元素的乘积 Element-Wise Product）。矩阵乘积，是指计算行与列之间的乘积之和。矩阵乘积，也称为 **点积 (dot product)**，计算如下：比如 **2×2** 的矩阵，其乘积可以如下图这样进行计算：\n",
    "\n",
    "<img src=\"./img/3_04.png\" width=\"80%\">\n",
    "\n",
    "使用 `np.dot()` 函数实现矩阵乘积，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19, 22],\n",
       "       [43, 50]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建数组\n",
    "A = np.array([[1,2], [3,4]])\n",
    "B = np.array([[5,6], [7,8]])\n",
    "\n",
    "# 执行点积运算\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在运算矩阵乘积时需要注意：\n",
    "\n",
    "1. 相乘的顺序很重要，AB != BA\n",
    "2. 如果第一个矩阵的列数（column），等于第二个矩阵的行数（row），则矩阵可以相乘\n",
    "\n",
    "再举一个例子 ——\n",
    "\n",
    "<img src=\"./img/4_04.png\" width=\"50%\">\n",
    "\n",
    "使用 `np.dot()` 函数实现矩阵乘积，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[140, 146],\n",
       "       [320, 335]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建数组\n",
    "A = np.array([[1,2,3], [4,5,6]])\n",
    "B = np.array([[10,11], [20,21], [30,31]])\n",
    "\n",
    "# 执行点积运算\n",
    "np.dot(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一个例子，将 **1x5** 与 **5x1** 矩阵相乘。\n",
    "\n",
    "<img src=\"./img/5_04.png\" width=\"60%\">\n",
    "\n",
    "使用 `np.dot()` 函数实现矩阵乘积，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[550]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建数组\n",
    "A = np.array([[1,2,3,4,5]])\n",
    "B = np.array([[10],\n",
    "              [20],\n",
    "              [30],\n",
    "              [40],     \n",
    "              [50]])\n",
    "\n",
    "# 执行点积运算\n",
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 神经网络的内积 (Inner product)\n",
    "\n",
    "让我们看一下典型神经网络中的单个神经元。它从其他单元接收输入 $\\left\\{x_{1}, \\ldots, x_{n}\\right\\}$  并产生输出 ***y***，为了计算输出，我们将每个输入与相应的权重 $\\left\\{w_{1}, \\ldots, w_{n}\\right\\}$ 进行乘积。权重值确定每个输入的连接强度。我们将权重值与输入的乘积求和，然后加上一个偏置（bias） ***b***，以获得输入的总量。最终的输出 ***y***，就是通过以输入总量作为参数，运算激活函数 ***f*** 而获得的。激活函数通常是非线性的，例如 **sigmoid** 和 **Relu**。因此，我们有该神经元的输出公式：\n",
    "\n",
    "$$\n",
    "y=f\\left(\\sum_{i=1}^{n} w_{i} x_{i}+b\\right)\n",
    "$$\n",
    "\n",
    "权重与输入的乘积求和，可以重写为点积，将向量 $\\left\\{x_{1}, \\ldots, x_{n}\\right\\}$ 定义为输入，向量 $\\left\\{w_{1}, \\ldots, w_{n}\\right\\}$ 定义为权重，通过点积定义有：\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} w_{i} x_{i}=w \\cdot x\n",
    "$$\n",
    "\n",
    "将其重新代入到到输出的公式中：\n",
    "\n",
    "$$\n",
    "y=f(w \\cdot x+b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 设置输入 {X1...Xn}\n",
    "x = np.array([1, 2]) \n",
    "# 显示x形状\n",
    "x.shape              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 5]\n",
      " [2 4 6]]\n"
     ]
    }
   ],
   "source": [
    "# 设置权重 {W1...Wn} ，\n",
    "# [[1, 3, 5],\n",
    "# [2, 4, 6]]\n",
    "# 注意 X 的列数等于 W 的行数\n",
    "w = np.array([[1, 3, 5], [2, 4, 6]]) \n",
    "# 输出权重值矩阵\n",
    "print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape  # 显示w形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 13 20]\n"
     ]
    }
   ],
   "source": [
    "# 设置偏置量b(bias)\n",
    "b = [1, 2, 3]        \n",
    "# 将点积加上偏置量 b，求出 y，\n",
    "y = np.dot(x, w) + b \n",
    "\n",
    "# 输出 \"[ 6 13 20]\"\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正因为点积的出现，我们不需要对每一个输入 **x** 与权重 **w** 进行逐一相乘，使用 **`np.dot`**（多维数组的点积函数），就可以一次性计算出 **y** 的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验目标\n",
    "\n",
    "本实验中，我们首先使用 NumPy 数组，用很少的代码，实现从神经网络输入到输出的（前向）处理。我们将实现这样一个三层神经网络：输入层（第 0 层）有两个神经元，第一个隐藏层（第 1 层）有三个神经元，第二个隐藏层（第 2 层）有两个神经元，输出层（第 3 层）有两个神经元。另外，我们在前面两个隐藏层会使用 **Sigmoid** 作为激活函数，而在最后的输出层则使用 **恒等函数** (直接输出)。\n",
    "\n",
    "![](./img/3layersnn_04.png)\n",
    "\n",
    "之后，我们将通过 MNIST 手写数字数据集体验案例，重新搭建一个简单而完整的三层神经网络，包含：输入层，隐含层和输出层，让你体验并实现将深度学习神经网络应用到手写数字识别中。MNIST 手写数字数据集是一个小规模的神经网络的训练和测试数据集，也是经典的深度学习的基准数据集，在深度学习的课程群中会被反复应用。本实验中的 MNIST 数据集所存储数据文件为 CSV 文件，每一个码值都是由逗号分隔，对于每行（最左边有行号）的数值，开头的第一个在值是标签，即书写者希望表示的数字，如第一行的 “5”、第二行的 “0”。随后的 **784** 个值，由逗号分隔，是手写数字的像素值，像素数组的尺寸是 **`28*28`**（即 `784`）,这 784 个像素值是手写数字的 **`28*28`** 的图片对应的 **784** 个位置的像素值。\n",
    "\n",
    "![](img/0_FLD5PNYJgWknPvJQ_04.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入numpy科学计算库，支持维度数组与矩阵运算\n",
    "import numpy as np             \n",
    "# 导入matplotlib绘图库\n",
    "import matplotlib.pylab as plt \n",
    "\n",
    "# 魔法命令，使matplotlib图形直接在jupyter notebook中显示\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 层间信号传递的实现\n",
    "\n",
    "图例约定如下：\n",
    "\n",
    "<img src=\"./img/8_04.png\" width=\"50%\">\n",
    "\n",
    "以输入层（第 0 层）到第一个隐藏层（第 1 层）的信号传递为例：\n",
    "\n",
    "<img src=\"./img/7_04.png\" width=\"50%\">\n",
    "\n",
    "其中 ——\n",
    "\n",
    "- $b_{1}^{(1)}$ 为第 1 层偏置量\n",
    "- **x<sub>1</sub>**，**x<sub>2</sub>** 为输入\n",
    "- $W_{11}^{(1)}$ 和 $W_{12}^{(1)}$ 分别为第 1 层对应输入层（第 0 层）第 1，第 2 个神经元，到第 1 层第 1 个神经元的权重值\n",
    "- $a_{1}^{(1)}$ 为输入层到第一个隐藏层（第 1 层）中的第一个神经元的输出值\n",
    "\n",
    "因此，$a_{1}^{(1)}$ 过加权信号和偏置的和，按如下方式进行计算：\n",
    "\n",
    "$$\n",
    "a_{1}^{(1)}=w_{11}^{(1)} x_{1}+w_{12}^{(1)} x_{2}+b_{1}^{(1)}\n",
    "$$\n",
    "\n",
    "同理，我们还需要算出 $a_{2}^{(1)}$ ，和 $a_{3}^{(1)}$ 的值，这样一来运算会很繁琐。\n",
    "\n",
    "由于我们有矩阵点积运算和 Numpy 的 **np.dot()** 函数，就可以对所有的输入、权重进行打包运算。因此，将第 1 层的加权和表示成下面的式：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A}^{(1)}=\\boldsymbol{X} \\boldsymbol{W}^{(1)}+\\boldsymbol{B}^{(1)}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\boldsymbol{A}^{(1)}=\\left(\\begin{array}{lll}\n",
    "a_{1}^{(1)} & a_{2}^{(1)} & a_{3}^{(1)}\n",
    "\\end{array}\\right), \\boldsymbol{X}=\\left(\\begin{array}{ll}\n",
    "x_{1} & x_{2}\n",
    "\\end{array}\\right), \\boldsymbol{B}^{(1)}=\\left(\\begin{array}{lll}\n",
    "b_{1}^{(1)} & b_{2}^{(1)} & b_{3}^{(1)}\n",
    "\\end{array}\\right) \\\\\n",
    "\\boldsymbol{W}^{(1)}=\\left(\\begin{array}{lll}\n",
    "w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\\\\n",
    "w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}\n",
    "\\end{array}\\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "这样一来，我们就可以通过一次运算将输入、权重、偏置值同时打包运算，从而运算出第一次所有神经元的加权和（一个数组）。\n",
    "\n",
    "下面我们用 NumPy 多维数组来实现，这里将输入信号、权重、偏置设置成任意值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2,)\n",
      "(3,)\n",
      "[0.3 0.7 1.1]\n"
     ]
    }
   ],
   "source": [
    "# 定义 x1，x2 输入\n",
    "X = np.array([1.0, 0.5]) \n",
    "\n",
    "# 定义第1层权重值，矩阵分别对应两个输入（x1，x2）和三个神经元（a1，a2，a3）\n",
    "W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])  \n",
    "\n",
    "# 定义第1层偏置值，对应三个第 1 层的神经元\n",
    "B1 = np.array([0.1, 0.2, 0.3])                     \n",
    "\n",
    "# (2, 3)\n",
    "print(W1.shape) \n",
    "# (2,)\n",
    "print(X.shape) \n",
    "# (3,)\n",
    "print(B1.shape) \n",
    "\n",
    "# 求第 1 层的加权和\n",
    "A1 = np.dot(X, W1) + B1    \n",
    "# 这里实际上包含了 a1，a2，a3 三个神经元的加权和\n",
    "# [0.3 0.7 1.1]\n",
    "print(A1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将加权和代入激活函数\n",
    "\n",
    "完成加权和的运算后，我们还需要将其代入激活函数，才真正得到该层的输出。从上面的代码运算中，你应该可以理解到，虽然在图中的为了讲解方便，用黑色线强调的是第 1 层的第 1 个神经元，但实际上在真正运算中是将 3 个神经元作为数组同时代入激活函数运算。\n",
    "\n",
    "<img src=\"./img/9_04.png\" width=\"50%\">\n",
    "\n",
    "隐藏层的加权和（加权信号和偏置的总和）用 **a** 表示，被激活函数转换后的信号用 **z** 表示。此外，图中 **h()** 表示激活函数，这里我们使用的是 **sigmoid** 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3 0.7 1.1]\n",
      "[0.57444252 0.66818777 0.75026011]\n"
     ]
    }
   ],
   "source": [
    "# 定义激活函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "# 将 A1 代入激活函数，使三个第 1 层的神经元 a1, a2, a3 的加权和同时代入激活函数运算\n",
    "Z1 = sigmoid(A1) \n",
    "\n",
    "#  代入参数 A1\n",
    "# [0.3, 0.7, 1.1]                            \n",
    "print(A1) \n",
    "#  激活函数 sigmoid 运算后的结果 \n",
    "# [0.57444252, 0.66818777, 0.75026011] \n",
    "print(Z1)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现第 1 层到第 2 层的信号传递\n",
    "\n",
    "除了将刚刚运算出来的第 1 层的输出（**Z1**）变成了第 2 层的输入以外，这个实现和刚才的代码完全相同。由此可知，通过使用 NumPy 数组，可以将层到层的信号传递过程简单地写出来。\n",
    "\n",
    "<img src=\"./img/10_04.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "(3, 2)\n",
      "(2,)\n",
      "[0.62624937 0.7710107 ]\n"
     ]
    }
   ],
   "source": [
    "# 定义第2层权重值，矩阵分别对应三个输入（z1，z2, z3）和两个神经元（a1，a2）\n",
    "W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) \n",
    "\n",
    "# 定义第2层偏置值，对应两个第 2 层的神经元\n",
    "B2 = np.array([0.1, 0.2])                           \n",
    "\n",
    "# (3,)\n",
    "print(Z1.shape) \n",
    "# (3, 2)\n",
    "print(W2.shape) \n",
    "# (2,)\n",
    "print(B2.shape) \n",
    "\n",
    "# 求第 2 层的加权和\n",
    "A2 = np.dot(Z1, W2) + B2  \n",
    "\n",
    "# 将 A2 代入激活函数，使两个第 2 层的神经元 a1, a2 的加权和同时代入激活函数运算\n",
    "Z2 = sigmoid(A2)          \n",
    "#  激活函数 sigmoid 运算后的结果 \n",
    "print(Z2)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现第 2 层到输出层的信号传递\n",
    "\n",
    "最后是第 2 层到输出层的信号传递。输出层的实现也和之前的实现基本相同。不过，这里我们定义了 `identity_function()` 函数（也称为 **恒等函数**），并将其作为输出层的激活函数。在数学里，恒等函数为一无任何作用的函数：它总是传回和其引数相同的值。换句话说，恒等函数为函数 ***f(x)=x***\n",
    "\n",
    "<img src=\"./img/11_04.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "# 定义恒等函数\n",
    "def identity_function(x):\n",
    "     return x\n",
    "\n",
    "# 定义第3层权重值，矩阵分别对应两个输入（z1，z2）和两个神经元（a1，a2）\n",
    "W3 = np.array([[0.1, 0.3], [0.2, 0.4]]) \n",
    "\n",
    "# 定义第3层偏置值，对应两个第 3 层（输出层）的神经元\n",
    "B3 = np.array([0.1, 0.2])               \n",
    "\n",
    "# 求第 3 层（输出层）的加权和\n",
    "A3 = np.dot(Z2, W3) + B3  \n",
    "# 或者Y = A3\n",
    "Y = identity_function(A3) \n",
    "# 输出 [ 0.31682708 0.69627909]\n",
    "print(Y)                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 代码小结与整理\n",
    "\n",
    "至此，我们已经介绍完了三层神经网络的实现。现在我们把之前的代码实现全部整理一下。这里，我们按照神经网络的实现惯例，只把权重记为大写字母 W1，其他的（偏置或中间结果等）都用小写字母表示。另外，我们定义了 `init_network()` 和 `forward()` 函数。\n",
    "\n",
    "- `init_network()` 函数：进行权重和偏置的初始化，并将它们保存在字典变量 network 中。这个字典变量 network 中保存了每一层所需的参数（权重和偏置）\n",
    "- `forward()` 函数：封装了将输入信号转换为输出信号的处理过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "def init_network():\n",
    "    network = {}\n",
    "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])\n",
    "    network['b1'] = np.array([0.1, 0.2, 0.3])\n",
    "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])\n",
    "    network['b2'] = np.array([0.1, 0.2])\n",
    "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])\n",
    "    network['b3'] = np.array([0.1, 0.2])\n",
    "    return network\n",
    "\n",
    "def forward(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "    \n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    \n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    \n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    \n",
    "    y = identity_function(a3)\n",
    "    return y\n",
    "\n",
    "network = init_network()\n",
    "x = np.array([1.0, 0.5])\n",
    "y = forward(network, x)\n",
    "\n",
    "# [ 0.31682708 0.69627909]\n",
    "print(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，最后得到与分部实现相同的运算结果。至此，你已成功实现了第一个三层神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 创建 MNIST 神经网络类\n",
    "\n",
    "在实验的第一部分，我们通过使用 sigmoid 激活函数完成了第一个三层神经网络的构建。然而，我们依然是通过手动配置输入、权重、偏置量等参数来进行运算；而实际上，真正的深度学习网络可以通过输入数据与输出结果之间的对比，自动得到各种参数 —— 这个过程就是 **模型训练**。\n",
    "\n",
    "在实验的第二部分，我们将让你体验并实现如何将深度学习神经网络应用到手写数字识别中，让你尽快对一个完整的深度学习神经网络的运行流程建立初步认知。我们对代码进行了详细的注释，但不会把所有的概念一次性灌输给你。在后续的课程中，我们将通过独立的实验操作，逐步使你深刻理解相关的概念。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入scipy科学计算库，支持大量科学运算函数\n",
    "import scipy.special           \n",
    "# 导入glob通配符查找库\n",
    "import glob                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码中，我们先补充两个在实验的第二部分应用到的两个库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNetwork :\n",
    "    \n",
    "    # 初始化神经网络\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learningrate) :\n",
    "        \n",
    "        # 设置输入层、隐含层和输出层节点数\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # 权重值\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes,-0.5), \n",
    "                                    (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes,-0.5), \n",
    "                                    (self.onodes, self.hnodes))\n",
    "        \n",
    "        # 学习率\n",
    "        self.lr = learningrate\n",
    "        \n",
    "        # sigmoid激活函数\n",
    "        self.activation_function = lambda x:scipy.special.expit(x)\n",
    "        pass\n",
    "    \n",
    "    # 训练模型\n",
    "    def train(self, inputs_list, targets_list) :\n",
    "        \n",
    "        # 将输入转换为2维数组\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # 计算隐藏层的输入\n",
    "        hidden_inputs = np.dot(self.wih,inputs)\n",
    "        \n",
    "        # 隐藏层输入通过激活函数变换\n",
    "        hidden_outputs = self.activation_function(hidden_inputs) \n",
    "        \n",
    "        # 计算输出\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # 计算输出的损失值\n",
    "        output_errors = targets - final_outputs\n",
    "        \n",
    "        # 计算隐藏层的损失值\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        # 更新隐藏层和输出层之间的权重值\n",
    "        self.who += self.lr*np.dot((output_errors*final_outputs*(1.0 - final_outputs)), \n",
    "                                   np.transpose(hidden_outputs))\n",
    "        \n",
    "        # 更新隐藏层和输入层之间的权重值\n",
    "        self.wih += self.lr*np.dot((hidden_errors*hidden_outputs*(1.0 - hidden_outputs)), \n",
    "                                   np.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # 查询神经网络\n",
    "    def query(self, inputs_list) :\n",
    "        \n",
    "        # 将输入列表转换为2维数组\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # 计算隐藏层的输入\n",
    "        hidden_inputs = np.dot(self.wih,inputs)\n",
    "        \n",
    "        # 隐藏层的激活函数变换\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        #计算输出层的输出值\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 784\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10\n",
    "\n",
    "# 学习率\n",
    "learning_rate = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 创建神经网络实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = neuralNetwork(input_nodes,hidden_nodes,output_nodes,learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 加载训练数据\n",
    "\n",
    "这里数据集为 `mnist_train_100.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置路径\n",
    "import os\n",
    "base_path = os.environ.get(\"BATH_PATH\",'./data/')\n",
    "data_path = os.path.join(base_path + \"lab4/\")\n",
    "result_path = \"result/\"\n",
    "os.makedirs(result_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_file = open(data_path+\"mnist_train_100.csv\",'r')\n",
    "training_data_list = training_data_file.readlines()\n",
    "training_data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in training_data_list:\n",
    "    all_values = record.split(',')\n",
    "    \n",
    "    # 缩放和变换输入\n",
    "    inputs = (np.asfarray(all_values[1:])/255.0*0.99)+0.01\n",
    "    \n",
    "    # 创建目标输出值（除了标签为0.99，其余的都为0.01）\n",
    "    targets = np.zeros(output_nodes)+0.01\n",
    "    targets[int(all_values[0])] = 0.99\n",
    "    n.train(inputs,targets)\n",
    "    n.query(inputs)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 测试训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03821187],\n",
       "       [0.01600562],\n",
       "       [0.0408104 ],\n",
       "       [0.03528962],\n",
       "       [0.06528017],\n",
       "       [0.03878796],\n",
       "       [0.01267818],\n",
       "       [0.85936343],\n",
       "       [0.04757153],\n",
       "       [0.08060646]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#加载测试数据\n",
    "test_data_file = open(data_path+\"mnist_test_10.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()\n",
    "\n",
    "#显示预测概率和图片\n",
    "all_values = test_data_list[0].split(',')\n",
    "image_array = np.asfarray(all_values[1:]).reshape((28,28))\n",
    "plt.imshow(image_array, cmap='Greys', interpolation = 'None')\n",
    "n.query((np.asfarray(all_values[1:]) / 255.0*0.99) + 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验小结\n",
    "\n",
    "在本实验中，你完成了神经网络的前向处理的实现。通过巧妙地使用 NumPy 多维数组，高效地实现了神经网络。另外，你还通过使用 MNIST 数据集，通过应用深度学习神经网络，实现了对手写数字的识别 —— 在实验的第二部分，你可能会碰到很多陌生的概念。没有关系，这里仅仅是为了让你尽快体验一个完整的深度学习神经网络的运行流程。我们会在后续的课程中逐个进行深入的介绍与实验，使你深刻理解相关概念。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
