{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验：Dropout 算法\n",
    "\n",
    "除了权重衰减以外，深度学习模型常常使用 **丢弃法（dropout）** 来应对过拟合问题。丢弃法有一些不同的变体。本实验中提到的丢弃法特指：**倒置丢弃法（inverted dropout）**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验概要\n",
    "\n",
    "#### Dropout 算法\n",
    "\n",
    "多层感知机在单层神经网络的基础上引入了一到多个隐藏层（hidden layer）。隐藏层位于输入层和输出层之间。\n",
    "\n",
    "![Alt text](./img/mlp_21.svg)\n",
    "\n",
    "设：输入个数为 `4`，隐藏单元个数为 `5`，且隐藏单元 $h_{i}(i=1, \\ldots, 5)$ 的计算表达式为：\n",
    "\n",
    "$$h_{i}=\\phi\\left(x_{1} w_{1 i}+x_{2} w_{2 i}+x_{3} w_{3 i}+x_{4} w_{4 i}+b_{i}\\right)$$\n",
    "\n",
    "这里 $\\phi$ 是激活函数，$x_{1}, \\ldots, x_{4}$ 是输入，隐藏单元 $i$ 的权重参数为 $w_{1 i}, \\ldots, w_{4 i}$，偏置参数为 $b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为 $p$，那么， $h_i$ 有 $p$ 的概率会被清零，有 $(1 − p)$ 的概率会除以 $(1 − p)$ 做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量 $\\xi_{i}$ 为 0 和 1 的概率分别为 $p$ 和 $(1 − p)$，使用丢弃法时我们计算新的隐藏单元 $h_{i}^{\\prime}$：\n",
    "\n",
    "$$\n",
    "h_{i}^{\\prime}=\\frac{\\xi_{i}}{1-p} h_{i}\n",
    "$$\n",
    "\n",
    "由于 $E\\left(\\xi_{i}\\right)=1-p$，因此：\n",
    "\n",
    "$$\n",
    "E\\left(h_{i}^{\\prime}\\right)=\\frac{E\\left(\\xi_{i}\\right)}{1-p} h_{i}=h_{i}\n",
    "$$\n",
    "\n",
    "即**丢弃法不改变其输入的期望值**。如对上图的隐藏层使用丢弃法，一种可能的结果如下图所示，其中 $h_{2}$ 和 $h_{5}$ 被清零。这时输出值的计算不再依赖 $h_{2}$ 和 $h_{5}$，在反向传播时，与这两个隐藏单元相关的权重的梯度均为 `0`。由于在训练中隐藏层神经元的丢弃是随机的，即 $h_{1}, \\ldots, h_{5}$ 都有可能被清零，输出层的计算无法过度依赖 $h_{1}, \\ldots, h_{5}$ 中的任一个，从而在训练模型时起到正则化的作用，并可以用来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，一般不使用丢弃法。\n",
    "\n",
    "![Alt text](./img/dropout_21.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验目标\n",
    "\n",
    "在本实验中，我们将根据丢弃法的定义，通过 Python 实现它，之后我们会通过 Tensorflow 框架作相同的测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras, nn, losses\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 定义 Dropout 函数\n",
    "\n",
    "我们将通过定义一个 `dropout()` 函数，以 `drop_prob` 的概率丢弃 X 中的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(X, drop_prob):\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    \n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return tf.zeros_like(X)\n",
    "    \n",
    "    #初始mask为一个bool型数组，故需要强制类型转换\n",
    "    mask = tf.random.uniform(shape=X.shape, minval=0, maxval=1) < keep_prob\n",
    "    return tf.cast(mask, dtype=tf.float32) * tf.cast(X, dtype=tf.float32) / keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们运行几个例子来测试一下 `dropout()` 函数。其中丢弃概率分别为 `0`、`0.5` 和 `1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=float32, numpy=\n",
       "array([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11., 12., 13., 14., 15.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.reshape(tf.range(0, 16), shape=(2, 8))\n",
    "dropout(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面全部隐藏层单元被保留。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=float32, numpy=\n",
       "array([[ 0.,  0.,  0.,  6.,  8., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0., 24., 26., 28.,  0.]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面部分隐藏层单元被保留，剩下的一部分在原有值的基础上除以 $(1 - p)$，我们设置了 `p = 0.5`，即乘以 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 8), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout(X, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面全部隐藏层单元被清零。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 定义模型参数\n",
    "\n",
    "下面，我们将定义一个包含两个隐藏层的多层感知机，其中两个隐藏层的输出个数都是 `256`，然后将其代入到 MNIST 数据集中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = tf.Variable(tf.random.normal(stddev=0.01, shape=(num_inputs, num_hiddens1)))\n",
    "b1 = tf.Variable(tf.zeros(num_hiddens1))\n",
    "W2 = tf.Variable(tf.random.normal(stddev=0.1, shape=(num_hiddens1, num_hiddens2)))\n",
    "b2 = tf.Variable(tf.zeros(num_hiddens2))\n",
    "W3 = tf.Variable(tf.random.truncated_normal(stddev=0.01, shape=(num_hiddens2, num_outputs)))\n",
    "b3 = tf.Variable(tf.zeros(num_outputs))\n",
    "\n",
    "params = [W1, b1, W2, b2, W3, b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定义模型\n",
    "\n",
    "下面定义的模型将全连接层和激活函数 `ReLU` 串起来，并对每个激活函数的输出使用丢弃法。我们可以分别设置各个层的丢弃概率。通常的建议是把靠近输入层的丢弃概率设得小一点。在这个实验中，我们把第一个隐藏层的丢弃概率设为 `0.2`，把第二个隐藏层的丢弃概率设为 `0.5`。我们可以通过参数 `is_training` 函数来判断运行模式为训练还是测试，并只需在训练模式下使用丢弃法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "\n",
    "def net(X, is_training=False):\n",
    "    X = tf.reshape(X, shape=(-1,num_inputs))\n",
    "    H1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    \n",
    "    # 只在训练模型时使用丢弃法\n",
    "    if is_training:\n",
    "        # 在第一层全连接后添加丢弃层\n",
    "        H1 = dropout(H1, drop_prob1)  \n",
    "    H2 = nn.relu(tf.matmul(H1, W2) + b2)\n",
    "    \n",
    "    if is_training:\n",
    "        # 在第二层全连接后添加丢弃层\n",
    "        H2 = dropout(H2, drop_prob2)  \n",
    "    \n",
    "    return tf.math.softmax(tf.matmul(H2, W3) + b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 训练和测试模型\n",
    "\n",
    "#### 5.1 定义准确率函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc_sum, n = 0.0, 0\n",
    "    for _, (X, y) in enumerate(data_iter):\n",
    "        y = tf.cast(y,dtype=tf.int64)\n",
    "        acc_sum += np.sum(tf.cast(tf.argmax(net(X), axis=1), dtype=tf.int64) == y)\n",
    "        n += y.shape[0]\n",
    "    return acc_sum / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 定义模型训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params=None, lr=None, trainer=None):\n",
    "    global sample_grads\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
    "        for X, y in train_iter:\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_hat = net(X, is_training=True)\n",
    "                l = loss(y_hat, tf.one_hot(y, depth=10, axis=-1, dtype=tf.float32))\n",
    "\n",
    "            grads = tape.gradient(l, params)\n",
    "            if trainer is None: # 如果没有传入优化器\n",
    "\n",
    "                sample_grads = grads\n",
    "                params[0].assign_sub(grads[0] * lr)\n",
    "                params[1].assign_sub(grads[1] * lr)\n",
    "            else:\n",
    "                trainer.apply_gradients(zip(grads, params)) \n",
    "\n",
    "            y = tf.cast(y, dtype=tf.float32)\n",
    "            train_l_sum += l.numpy()\n",
    "            train_acc_sum += tf.reduce_sum(tf.cast(tf.argmax(y_hat, axis=1) \n",
    "                                == tf.cast(y, dtype=tf.int64), dtype=tf.int64)).numpy()\n",
    "            n += y.shape[0]\n",
    "        test_acc = evaluate_accuracy(test_iter, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\uparrow$ 由于 Notebook 输出格式限制，其中可能有代码显示不全：\n",
    "\n",
    "```python\n",
    "train_acc_sum += tf.reduce_sum(tf.cast(tf.argmax(y_hat, axis=1) == tf.cast(y, dtype=tf.int64), dtype=tf.int64)).numpy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 加载数据\n",
    "\n",
    "MNIST 是 TensorFlow 的内置数据集，直接从框架中导入即可。\n",
    "\n",
    "```keras.datasets.mnist.load_data()```\n",
    "\n",
    "但由于需要从互联网上下载，连接可能不稳定，我们直接下载到本地，使用下面的代码进行加载。\n",
    "\n",
    "与之前实验一样，训练集与测试据依然是 60000 张 / 10000 张 `28*28` 像素图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = os.environ.get(\"BATH_PATH\",'./data/')\n",
    "data_path = os.path.join(base_path + \"lab21/\")\n",
    "result_path = \"result/\"\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "path = data_path+'mnist.npz'\n",
    "\n",
    "f = np.load(path)\n",
    "x_train, y_train = f['x_train'], f['y_train']\n",
    "x_test, y_test = f['x_test'], f['y_test']\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0387, train acc 0.582, test acc 0.758\n",
      "epoch 2, loss 0.0242, train acc 0.727, test acc 0.840\n",
      "epoch 3, loss 0.0198, train acc 0.781, test acc 0.865\n",
      "epoch 4, loss 0.0176, train acc 0.802, test acc 0.877\n",
      "epoch 5, loss 0.0162, train acc 0.816, test acc 0.885\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.CategoricalCrossentropy()\n",
    "num_epochs, lr, batch_size = 5, 0.5, 256\n",
    "\n",
    "# 在进行矩阵相乘时需要 float 型，故强制类型转换为 float 型\n",
    "x_train = tf.cast(x_train, tf.float32) / 255 \n",
    "\n",
    "# 在进行矩阵相乘时需要 float 型，故强制类型转换为 float 型\n",
    "x_test = tf.cast(x_test,tf.float32) / 255 \n",
    "train_iter = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "test_iter = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "train_mlp(net, train_iter, test_iter, loss, num_epochs, batch_size,\n",
    "              params, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上面的训练结果看到训练集的准确率 `train acc`，与测试集的准确率 `test acc`，基本匹配，并没有出现前面的实验中偏离值很严重的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 使用 TensorFlow 框架实现训练\n",
    "\n",
    "在 Tensorflow2.0 中，我们只需要在全连接层后添加 Dropout 层并指定丢弃概率。在训练模型时，Dropout 层将以指定的丢弃概率随机丢弃上一层的输出元素；在测试模型时（即 `model.eval()` 后），Dropout 层并不发挥作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(256,activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    keras.layers.Dense(256,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    keras.layers.Dense(10,activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面训练并测试模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "235/235 [==============================] - 6s 21ms/step - loss: 0.4341 - accuracy: 0.8709 - val_loss: 0.1526 - val_accuracy: 0.9522\n",
      "Epoch 2/5\n",
      "235/235 [==============================] - 5s 19ms/step - loss: 0.1741 - accuracy: 0.9488 - val_loss: 0.1073 - val_accuracy: 0.9653\n",
      "Epoch 3/5\n",
      "235/235 [==============================] - 5s 20ms/step - loss: 0.1234 - accuracy: 0.9632 - val_loss: 0.0867 - val_accuracy: 0.9740\n",
      "Epoch 4/5\n",
      "235/235 [==============================] - 4s 18ms/step - loss: 0.1003 - accuracy: 0.9688 - val_loss: 0.0771 - val_accuracy: 0.9764\n",
      "Epoch 5/5\n",
      "235/235 [==============================] - 4s 19ms/step - loss: 0.0831 - accuracy: 0.9747 - val_loss: 0.0734 - val_accuracy: 0.9775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fef7e92ab70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train,y_train,epochs=5,\n",
    "          batch_size=256,\n",
    "          validation_data=(x_test, y_test),\n",
    "          validation_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，在 Tensorflow 框架中实现 Dropout，训练集的准确率 `accuracy` 与测试集的准曲率 `val_accuracy` 依然匹配，也没有出现前面的实验中偏离值很严重的情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验小结\n",
    "\n",
    "在本实验中，你根据丢弃法的定义，通过 Python 实现了 Dropout 运算，之后又通过 Tensorflow 框架作相同的测试。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
