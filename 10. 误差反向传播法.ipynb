{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验：误差反向传播法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们需要对误差反向传播法（简称：**backpropagation，反向传播**）形成直观而专业的理解。反向传播，是利用 **链式法则** (chain rule) 递归计算表达式的梯度的方法。理解反向传播过程及其精妙之处，对于理解、实现、设计和调试神经网络非常关键。我们要解决的核心问题是：给定函数 $f(x)$ ，其中 $x$ 是输入数据的向量，需要计算函数 $f$ 关于 $x$ 的梯度，也就是 $\\nabla f(x)$。\n",
    "\n",
    "之所以关注上述问题，是因为在神经网络中 $f$ 对应的是损失函数 $(L)$ ，输入 $x$ 里面包含训练数据和神经网络的权重。举个例子，损失函数可以是 SVM 的损失函数，输入则包含了训练数据 $\\left(x_{i}, y_{i}\\right), i=1 \\ldots N$、权重 $W$ 和偏置 $b$。注意训练集是给定的（在机器学习中通常都是这样），而权重是可以控制的变量。因此，即使能用反向传播计算输入数据 $x_{i}$ 上的梯度，但在实践为了进行参数更新，通常也只计算参数（比如：权重 $W$ 和偏置 $b$）的梯度。然而 $x_{i}$ 的梯度有时仍然是有用的：比如将神经网络所做的事情可视化便于直观理解的时候，就能用上。\n",
    "\n",
    "下面，我们首先呈现了一个相对成熟的反向传播视角，在该视角中能看见基于实数值回路的反向传播过程，而对其细节的理解和收获将帮助你更好地通过本课程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 简单表达式和理解梯度\n",
    "\n",
    "从简单表达式入手可以为复杂表达式打好符号和规则基础。先考虑一个简单的二元乘法函数 $f(x, y)=x y$。对两个输入变量分别求偏导数还是很简单的：\n",
    "\n",
    "$$\n",
    "f(x, y)=x y \\rightarrow \\frac{d f}{d x}=y \\quad \\frac{d f}{d y}=x\n",
    "$$\n",
    "\n",
    "**解释：** 牢记这些导数的意义：函数变量在某个点周围的极小区域内变化，而导数就是变量变化导致的函数在该方向上的变化率。\n",
    "\n",
    "$$\n",
    "\\frac{d f(x)}{d x}=\\lim _{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}\n",
    "$$\n",
    "\n",
    "注意等号左边的分号和等号右边的分号不同，不是代表分数。相反，这个符号表示操作符 $\\frac{d}{d x}$ 被应用于函数 $f$，并返回一个不同的函数（导数）。对于上述公式，可以认为 $h$ 值非常小，函数可以被一条直线近似，而导数就是这条直线的斜率。换句话说，每个变量的导数指明了整个表达式对于该变量的值的敏感程度。比如，若 $x=4$, $y=-3$ ，则 $f(x, y)=-12$，$x$ 的导数 $\\frac{\\partial f}{\\partial x}=-3$。这就说明如果将变量 $x$ 的值变大一点，整个表达式的值就会变小（原因在于负号），而且变小的量是 $x$ 变大的量的三倍。通过重新排列公式可以看到这一点（$f(x+h)=f(x)+h \\frac{d f(x)}{d x}$）。同样，因为 $\\frac{\\partial f}{\\partial y}=4$，可以知道如果将 $y$ 的值增加 $h$，那么函数的输出也将增加（原因在于正号），且增加量是 $4h$。\n",
    "\n",
    "> 函数关于每个变量的导数指明了整个表达式对于该变量的敏感程度。\n",
    "\n",
    "如上所述，梯度 $\\nabla f(x)$ 是偏导数的向量，所以有 $\\nabla f(x)=\\left[\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right]=[y, x]$。即使是梯度实际上是一个向量，仍然通常使用类似 “x 上的梯度” 的术语，而不是使用如 “x 的偏导数” 的正确说法，原因是因为前者说起来简单。\n",
    "\n",
    "我们也可以对加法操作求导：\n",
    "\n",
    "$$\n",
    "f(x, y)=x+y \\rightarrow \\frac{d f}{d x}=1 \\quad \\frac{d f}{d y}=1\n",
    "$$\n",
    "\n",
    "这就是说，无论其值如何，$x,y$ 的导数均为 1。这是有道理的，因为无论增加 $x,y$ 中任一个的值，函数 $f$ 的值都会增加，并且增加的变化率独立于 $x,y$ 的具体值（情况和乘法操作不同）。取最大值操作也是常常使用的：\n",
    "\n",
    "$$\n",
    "f(x, y)=\\max (x, y) \\rightarrow \\frac{d f}{d x}=1(x>=y) \\quad \\frac{d f}{d y}=1(y>=x)\n",
    "$$\n",
    "\n",
    "上式是说，如果该变量比另一个变量大，那么梯度是 1，反之为 0。例如，若 $x=4$, $y=2$，那么 max 是 4，所以函数对于 $y$ 就不敏感。也就是说，在 $y$ 上增加 $h$ ，函数还是输出为 4，所以梯度是 0：因为对于函数输出是没有效果的。当然，如果给 $y$ 增加一个很大的量，比如大于 2，那么函数 $f$ 的值就变化了，但是导数并没有指明输入量有巨大变化情况对于函数的效果，他们只适用于输入量变化极小时的情况，因为定义已经指明：$l i m_{h \\rightarrow 0}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用链式法则计算复合表达式\n",
    "\n",
    "在考虑更复杂的包含多个函数的复合函数，比如 $f(x, y, z)=(x+y) z$。虽然这个表达足够简单，可以直接微分，但是在此使用一种有助于读者直观理解反向传播的方法。将公式分成两部分：$q = x + y$ 和 $f = qz$。在前面已经介绍过如何对这分开的两个公式进行计算，因为 $f$ 是 $q$ 和 $z$ 相乘，所以 $\\frac{\\partial f}{\\partial q}=z, \\frac{\\partial f}{\\partial z}=q$，又因为 $q$ 是 $x$ 加 $y$，所以$\\frac{\\partial q}{\\partial x}=1, \\frac{\\partial q}{\\partial y}=1$。然而，并不需要关心中间量 $q$ 的梯度，因为 $\\frac{\\partial f}{\\partial q}$ 没有用。相反，函数 $f$ 关于 $x, y, z$ 的梯度才是需要关注的。**链式法则** 指出将这些梯度表达式链接起来的正确方式是相乘，比如 $\\frac{\\partial f}{\\partial x}=\\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}$。在实际操作中，这只是简单地将两个梯度数值相乘，示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置输入值\n",
    "x = -2; y = 5; z = -4\n",
    "\n",
    "# 进行前向传播\n",
    "# q becomes 3\n",
    "q = x + y \n",
    "# f becomes -12\n",
    "f = q * z \n",
    "\n",
    "# 进行反向传播:\n",
    "# 首先回传到 f = q * z\n",
    "# df/dz = q, 所以关于z的梯度是3\n",
    "dfdz = q \n",
    "# df/dq = z, 所以关于q的梯度是-4\n",
    "dfdq = z \n",
    "\n",
    "# 现在回传到q = x + y\n",
    "# dq/dx = 1. 这里的乘法是因为链式法则\n",
    "dfdx = 1.0 * dfdq \n",
    "# dq/dy = 1\n",
    "dfdy = 1.0 * dfdq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后得到变量的梯度 **`[dfdx, dfdy, dfdz]`**，它们告诉我们函数 $f$ 对于变量 **`[x, y, z]`** 的敏感程度。这是一个最简单的反向传播。一般会使用一个更简洁的表达符号，这样就不用写 `df` 了。这就是说，用 `dq` 来代替 `dfdq`，且总是假设梯度是关于最终输出的。这次计算可以被可视化为如下计算线路图像：\n",
    "\n",
    "![](./img/1_10.jpg)\n",
    "\n",
    "上图的真实值计算线路展示了计算的视觉化过程。**前向传播** 从输入计算到输出（绿色），**反向传播** 从尾部开始，根据链式法则递归地向前计算梯度（显示为红色），一直到网络的输入端。可以认为，梯度是从计算链路中回流。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播的直观理解\n",
    "\n",
    "反向传播是一个优美的局部过程。在整个计算线路图中，每个门单元都会得到一些输入并立即计算两个东西：1. 这个门的输出值，和 2.其输出值关于输入值的局部梯度。门单元完成这两件事是完全独立的，它不需要知道计算线路中的其他细节。然而，一旦前向传播完毕，在反向传播的过程中，门单元门将最终获得整个网络的最终输出值在自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对该门单元的每个输入值的梯度。\n",
    "\n",
    "> 这里对于每个输入的乘法操作是基于链式法则的。该操作让一个相对独立的门单元变成复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等。\n",
    "\n",
    "下面通过例子来对这一过程进行理解。加法门收到了输入 **`[-2, 5]`**，计算输出是 3。既然这个门是加法操作，那么对于两个输入的局部梯度都是 +1。网络的其余部分计算出最终值为 -12。在反向传播时将递归地使用链式法则，算到加法门（是乘法门的输入）的时候，知道加法门的输出的梯度是 -4。如果网络如果想要输出值更高，那么可以认为它会想要加法门的输出更小一点（因为负号），而且还有一个 4 的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让 -4 乘以 **x** 和 **y** 的局部梯度，x和y的局部梯度都是 1，所以最终都是 -4）。可以看到得到了想要的效果：如果 **x，y 减小**（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。\n",
    "\n",
    "因此，反向传播可以看做是门单元之间在通过梯度信号相互通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上升或降低，都是为了让整个网络的输出值更高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模块化：Sigmoid 例子\n",
    "\n",
    "上面介绍的门是相对随意的。任何可微分的函数都可以看做门。可以将多个门组合成一个门，也可以根据需要将一个函数分拆成多个门。现在看看一个表达式：\n",
    "\n",
    "$$\n",
    "f(w, x)=\\frac{1}{1+e^{-\\left(w_{0} x_{0}+w_{1} x_{1}+w_{2}\\right)}}\n",
    "$$\n",
    "\n",
    "在后面的课程中可以看到，这个表达式描述了一个含输入 $x$ 和权重 $w$ 的 2 维的神经元，该神经元使用了 **sigmoid 激活函数**。但是现在只是看做是一个简单的输入为 x 和 w，输出为一个数字的函数。这个函数是由多个门组成的。除了上文介绍的加法门，乘法门，取最大值门，还有下面这 4 种：\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "f(x)=\\frac{1}{x} \\rightarrow \\frac{d f}{d x}=-1 / x^{2} \\\\\n",
    "f_{c}(x)=c+x \\rightarrow \\frac{d f}{d x}=1 f(x)=e^{x} \\rightarrow \\frac{d f}{d x}=e^{x} \\\\\n",
    "f_{a}(x)=a x \\rightarrow \\frac{d f}{d x}=a\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "其中，函数 $f_{c}$ 使用对输入值进行了常量 $c$ 的平移， $f_{a}$ 将输入值扩大了常量 $a$ 倍。它们是加法和乘法的特例，但是这里将其看做一元门单元，因为确实需要计算常量 $c, a$ 的梯度。整个计算线路如下：\n",
    "\n",
    "![](./img/2_10.jpg)\n",
    "\n",
    "使用 **sigmoid 激活函数** 的 2 维神经元的例子。输入是 `[x0, x1]`，可学习的权重是 `[w0, w1, w2]`。一会儿会看见，这个神经元对输入数据做点积运算，然后其激活数据被 **sigmoid 函数**挤压到 0 到 1 之间。\n",
    "\n",
    "在上面的例子中可以看见一个函数操作的长链条，链条上的门都对 w 和 x 的点积结果进行操作。该函数被称为 **sigmoid 激活函数** $\\sigma(x)$。 **sigmoid 激活函数** 关于其输入的求导是可以简化的(使用了在分子上先加后减 1 的技巧)：\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\sigma(x)=\\frac{1}{1+e^{-x}} \\\\\n",
    "\\rightarrow \\frac{d \\sigma(x)}{d x}=\\frac{e^{-x}}{\\left(1+e^{-x}\\right)^{2}}=\\left(\\frac{1+e^{-x}-1}{1+e^{-x}}\\right)\\left(\\frac{1}{1+e^{-x}}\\right)=(1-\\sigma(x)) \\sigma(x)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "可以看到梯度计算简单了很多。举个例子， **sigmoid** 表达式输入为 1.0，则在前向传播中计算出输出为 0.73。根据上面的公式，局部梯度为: **`(1-0.73)*0.73~=0.2`**，和之前的计算流程比起来，现在的计算使用一个单独的简单表达式即可。因此，在实际的应用中将这些操作装进一个单独的门单元中将会非常有用。该神经元反向传播的代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# 假设一些随机数据和权重\n",
    "w = [2,-3,-3] \n",
    "x = [-1, -2]\n",
    "\n",
    "# 前向传播\n",
    "dot = w[0]*x[0] + w[1]*x[1] + w[2]\n",
    "# sigmoid函数\n",
    "f = 1.0 / (1 + math.exp(-dot)) \n",
    "\n",
    "# 对神经元反向传播\n",
    "# 点积变量的梯度, 使用sigmoid函数求导\n",
    "ddot = (1 - f) * f \n",
    "# 回传到x\n",
    "dx = [w[0] * ddot, w[1] * ddot] \n",
    "# 回传到w\n",
    "dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] \n",
    "# 完成！得到输入的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**实现提示：分段反向传播**。上面的代码展示了在实际操作中，为了使反向传播过程更加简洁，把向前传播分成不同的阶段将是很有帮助的。比如我们创建了一个中间变量 **dot**，它装着 **w** 和 **x** 的点乘结果。在反向传播的时，就可以（反向地）计算出装着 **w** 和 **x** 等的梯度的对应的变量（比如 **ddot**，**dx** 和 **dw**）。\n",
    "\n",
    "本节的要点就是展示反向传播的细节过程，以及前向传播过程中，哪些函数可以被组合成门，从而可以进行简化。知道表达式中哪部分的局部梯度计算比较简洁非常有用，这样他们可以“链”在一起，让代码量更少，效率更高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播实践：分段计算\n",
    "\n",
    "看另一个例子。假设有如下函数：\n",
    "\n",
    "$$\n",
    "f(x, y)=\\frac{x+\\sigma(y)}{\\sigma(x)+(x+y)^{2}}\n",
    "$$\n",
    "\n",
    "首先要说的是，这个函数完全没用，读者是不会用到它来进行梯度计算的，这里只是用来作为实践反向传播的一个例子，需要强调的是，如果对 $x$ 或 $y$ 进行微分运算，运算结束后会得到一个巨大而复杂的表达式。然而做如此复杂的运算实际上并无必要，因为我们不需要一个明确的函数来计算梯度，只需知道如何使用反向传播计算梯度即可。下面是构建前向传播的代码模式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 3 # 例子数值\n",
    "y = -4\n",
    "\n",
    "# 前向传播\n",
    "# 分子中的sigmoi\n",
    "sigy = 1.0 / (1 + math.exp(-y))          #(1)\n",
    "# 分子   \n",
    "num = x + sigy                           #(2)\n",
    "# 分母中的sigmoid\n",
    "sigx = 1.0 / (1 + math.exp(-x))          #(3)\n",
    "xpy = x + y                              #(4)\n",
    "xpysqr = xpy**2                          #(5)\n",
    "# 分母\n",
    "den = sigx + xpysqr                      #(6)\n",
    "invden = 1.0 / den                       #(7)\n",
    "# 搞定！\n",
    "f = num * invden                         #(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到了表达式的最后，就完成了前向传播。注意在构建代码s时创建了多个中间变量，每个都是比较简单的表达式，它们计算局部梯度的方法是已知的。这样计算反向传播就简单了：我们对前向传播时产生每个变量 (**sigy, num, sigx, xpy, xpysqr, den, invden**) 进行回传。我们会有同样数量的变量，但是都以 **d** 开头，用来存储对应变量的梯度。注意在反向传播的每一小块中都将包含了表达式的局部梯度，然后根据使用链式法则乘以上游梯度。对于每行代码，我们将指明其对应的是前向传播的哪部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回传 f = num * invden\n",
    "# 分子的梯度 \n",
    "dnum = invden                                         #(8)\n",
    "dinvden = num                                         #(8)\n",
    "\n",
    "# 回传 invden = 1.0 / den \n",
    "dden = (-1.0 / (den**2)) * dinvden                    #(7)\n",
    "\n",
    "# 回传 den = sigx + xpysqr\n",
    "dsigx = (1) * dden                                    #(6)\n",
    "dxpysqr = (1) * dden                                  #(6)\n",
    "\n",
    "# 回传 xpysqr = xpy**2\n",
    "dxpy = (2 * xpy) * dxpysqr                            #(5)\n",
    "\n",
    "# 回传 xpy = x + y\n",
    "dx = (1) * dxpy                                       #(4)\n",
    "dy = (1) * dxpy                                       #(4)\n",
    "\n",
    "# 回传 sigx = 1.0 / (1 + math.exp(-x))\n",
    "# Notice += !! See notes below\n",
    "dx += ((1 - sigx) * sigx) * dsigx                     #(3)\n",
    "\n",
    "# 回传 num = x + sigy\n",
    "dx += (1) * dnum                                      #(2)\n",
    "dsigy = (1) * dnum                                    #(2)\n",
    "\n",
    "# 回传 sigy = 1.0 / (1 + math.exp(-y))\n",
    "dy += ((1 - sigy) * sigy) * dsigy                     #(1)\n",
    "# 完成！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意：\n",
    "\n",
    "- **对前向传播变量进行缓存：** 在计算反向传播时，前向传播过程中得到的一些中间变量非常有用。在实际操作中，最好代码实现对于这些中间变量的缓存，这样在反向传播的时候也能用上它们。如果这样做过于困难，也可以（但是浪费计算资源）重新计算它们。\n",
    "- **在不同分支的梯度要相加：** 如果变量 x，y 在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用 **`+=`** 而不是 **`=`** 来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的 **多元链式法则**，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应该进行累加。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 回传流中的模式\n",
    "\n",
    "一个有趣的现象是在多数情况下，反向传播中的梯度可以被很直观地解释。例如神经网络中最常用的加法、乘法和取最大值这三个门单元，它们在反向传播过程中的行为都有非常简单的解释。先看下面这个例子：\n",
    "\n",
    "![](./img/3_10.jpg)\n",
    "\n",
    "一个展示反向传播的例子。加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度。\n",
    "\n",
    "从上例可知：\n",
    "\n",
    "- 加法门单元把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的 +1，所以所有输入的梯度实际上就等于输出的梯度，因为乘以 1.0 保持不变。上例中，加法门把梯度 2.00 不变且相等地路由给了两个输入。\n",
    "\n",
    "- 取最大值门单元对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是 1.0，其余的是 0。上例中，取最大值门将梯度 2.00 转给了 **z** 变量，因为 **z** 的值比 **w** 高，于是 **w** 的梯度保持为 0。\n",
    "\n",
    "- 乘法门单元相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，**x** 的梯度是 `-4.00 x .00 = -8.00`。\n",
    "\n",
    "非直观影响及其结果。注意一种比较特殊的情况，如果乘法门单元的其中一个输入非常小，而另一个输入非常大，那么乘法门的操作将会不是那么直观：它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。在线性分类器中，权重和输入是进行点积 $w^{T} x_{i}$，这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本 $x_{i}$ 乘以 1000，那么权重的梯度将会增大 1000 倍，这样就必须降低学习率来弥补。这就是为什么数据预处理关系重大，它即使只是有微小变化，也会产生巨大影响。对于梯度在计算线路中是如何流动的有一个直观的理解，可以帮助你调试网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 用向量化操作计算梯度\n",
    "\n",
    "上述内容考虑的都是单个变量情况，但是所有概念都适用于矩阵和向量操作。然而，在操作的时候要注意关注维度和转置操作。\n",
    "\n",
    "**矩阵相乘的梯度：** 可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 前向传播\n",
    "W = np.random.randn(5, 10)\n",
    "X = np.random.randn(10, 3)\n",
    "D = W.dot(X)\n",
    "\n",
    "# 假设我们得到了D的梯度\n",
    "# 和D一样的尺寸\n",
    "dD = np.random.randn(*D.shape) \n",
    "#.T就是对矩阵进行转置\n",
    "dW = dD.dot(X.T) \n",
    "dX = W.T.dot(dD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：要分析维度！注意不需要去记忆 **dW** 和 **dX** 的表达，因为它们很容易通过维度推导出来。例如，权重的梯度 **dW** 的尺寸肯定和权重矩阵 **W** 的尺寸是一样的，而这又是由 **X** 和 **dD** 的矩阵乘法决定的（在上面的例子中X和W都是数字不是矩阵）。总有一个方式是能够让维度之间能够对的上的。例如，**X** 的尺寸是 **`[10x3]`**，**dD** 的尺寸是 **`[5x3]`** ，如果你想要 **dW** 和 **W** 的尺寸是 **`[5x10]`**，那就要 **dD.dot(X.T)**。\n",
    "\n",
    "使用小而具体的例子：有些读者可能觉得向量化操作的梯度计算比较困难，建议是写出一个很小很明确的向量化例子，在纸上演算梯度，然后对其一般化，得到一个高效的向量化操作形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验目标\n",
    "\n",
    "在本实验中，数据集有 `4200` 个样本标记了 `10` 个特征，我们使用多层感知器和反向传播法来更新权重使得在每次迭代中得到更高的准确率。在很多迭代过程中往往趋向于过拟合，过拟合在统计学中表现为低偏差，高方差。过拟合导致在测试集上不能取得好的效果。所以我们需要找到最好的权衡不同数量 `epechs` 的偏差和方差。另外，我们还有一些参数优化，如学习率、动量和最重要的激活函数的选择，在这里我们选择 `Relu`，`sigmoid` 和 `tanh`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 加载数据\n",
    "\n",
    "导入实验数据集为 `train.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = os.environ.get(\"BATH_PATH\",'./data/')\n",
    "data_path = os.path.join(base_path + \"lab10/\")\n",
    "result_path = \"result/\"\n",
    "os.makedirs(result_path, exist_ok=True)\n",
    "df = pd.read_csv(data_path+'train_10.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 训练数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1  对标签列进行 one-hot 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 794 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       0       0       0       0       0       0       0       0       0   \n",
       "1       0       0       0       0       0       0       0       0       0   \n",
       "2       0       0       0       0       0       0       0       0       0   \n",
       "3       0       0       0       0       0       0       0       0       0   \n",
       "4       0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9  ...  0  1  2  3  4  5  6  7  8  9  \n",
       "0       0  ...  0  1  0  0  0  0  0  0  0  0  \n",
       "1       0  ...  1  0  0  0  0  0  0  0  0  0  \n",
       "2       0  ...  0  1  0  0  0  0  0  0  0  0  \n",
       "3       0  ...  0  0  0  0  1  0  0  0  0  0  \n",
       "4       0  ...  1  0  0  0  0  0  0  0  0  0  \n",
       "\n",
       "[5 rows x 794 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label= pd.get_dummies(df['label'])\n",
    "df = pd.concat([df,label],axis=1)\n",
    "df.drop(['label'],inplace=True,axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 将 dataframe 转换为矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 选择训练样本特征列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = data[:1000,:784]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 选择训练样本标签列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = data[:1000,784:]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 选择验证样本特征列和标签列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = data[1000:1500,:784]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = data[1000:1500,784:]\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 定义激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid 函数\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid 函数导数\n",
    "def der_sigmoid(x):\n",
    "    return x * (1.0-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh函数，在主函数中并未调用\n",
    "def tanh(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu函数，在主函数中并未调用\n",
    "def relu(X):\n",
    "    return np.maximum(0,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu函数导数，在主函数中并未调用\n",
    "def der_relu(X):\n",
    "    return np.where(X <= 0,0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 神经网络参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 权重随机初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_1 = np.random.uniform(-1,1,(784,90))\n",
    "wb1=np.random.uniform(0,1,(1,90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2_2 = np.random.uniform(-1,1,(90,70))\n",
    "wb2=np.random.uniform(0,1,(1,70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3_3 = np.random.uniform(-1,1,(70,10))\n",
    "wb3=np.random.uniform(0,1,(1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 动量初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel1=np.zeros_like(w1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel2=np.zeros_like(w2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel3=np.zeros_like(w3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 偏置初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1=np.zeros((1000,1))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2=np.zeros((1000,1))+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "b3=np.zeros((1000,1))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 备份初始化权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1=np.copy(w1_1)\n",
    "w2=np.copy(w2_2)\n",
    "w3=np.copy(w3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 正向传播得到损失值，之后进行反向传播\n",
    "\n",
    "> **训练预计需时 15 分钟，请耐心等待。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the prediction:\n",
      " [[0.06890834 0.05084239 0.07695353 0.02009392 0.11195789 0.04350437\n",
      "  0.00627338 0.03452561 0.09055768 0.01473889]\n",
      " [0.17594803 0.01324235 0.0074274  0.18288357 0.00882942 0.05472467\n",
      "  0.06227405 0.01655663 0.08481746 0.03069111]\n",
      " [0.00559872 0.10344443 0.17528854 0.13992711 0.06151938 0.05372242\n",
      "  0.01757697 0.04402606 0.01296064 0.18499704]\n",
      " [0.16317914 0.06349532 0.19324548 0.29998135 0.04397487 0.01426107\n",
      "  0.34792055 0.02114863 0.03925485 0.0280708 ]]\n",
      "\n",
      " the output:\n",
      " [[0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]]\n",
      "the cost:\n",
      " 411.72951911866386\n",
      "训练总耗时： 5.030440807342529 秒\n"
     ]
    }
   ],
   "source": [
    "# 记录模型训练时间\n",
    "import time \n",
    "# 记录训练开始时间\n",
    "start_sum = time.time() \n",
    "\n",
    "lis = []\n",
    "for i in range(300):\n",
    "\n",
    "    #正向传播\n",
    "    \n",
    "    #第二层输出\n",
    "    z2=np.dot(X_train,w1)+np.dot(b1,wb1)\n",
    "    a2=sigmoid(z2)\n",
    "    \n",
    "    #第三层输出\n",
    "    z3=np.dot(a2,w2)+np.dot(b2,wb2)\n",
    "    a3=sigmoid(z3)\n",
    "  \n",
    "    #第四层输出\n",
    "    z4=np.dot(a3,w3)+np.dot(b3,wb3)\n",
    "    a4=sigmoid(z4)\n",
    "    \n",
    "    \n",
    "    #反向传播\n",
    "    \n",
    "    #计算损失值\n",
    "    cost = 0.5 * (y_train - a4)**2\n",
    "    lis.append(np.sum(cost))\n",
    "    \n",
    "    #方向求导\n",
    "    delta4= (y_train - a4) * (der_sigmoid(a4))\n",
    "    gamma3=np.dot(a3.T,delta4)\n",
    "        \n",
    "    delta3 = np.dot(delta4,w3.T) * (der_sigmoid(a3))\n",
    "    gamma2 = np.dot(a2.T,delta3)\n",
    "    \n",
    "    delta2 = np.dot(delta3,w2.T) * (der_sigmoid(a2))\n",
    "    gamma1 = np.dot(X_train.T, delta2) \n",
    "\n",
    "    #学习率\n",
    "    q=0.3\n",
    "    # 增加设置momentum动量值\n",
    "    momen=0.1 \n",
    "    \n",
    "    vel1=momen*vel1+q*(gamma1)\n",
    "    vel2=momen*vel2+q*(gamma2)\n",
    "    vel3=momen*vel3+q*(gamma3)\n",
    "    \n",
    "    #权重更新\n",
    "    w1= w1+(1/1000)*vel1\n",
    "    w2= w2+(1/1000)*vel2\n",
    "    w3= w3+(1/1000)*vel3\n",
    "\n",
    "\n",
    "print('the prediction:\\n',a4[:4])\n",
    "print('\\n the output:\\n',y_train[:4])\n",
    "print('the cost:\\n',np.sum(cost))\n",
    "\n",
    "# 记录训练结束时间\n",
    "end_sum = time.time() \n",
    "print('训练总耗时：',end_sum - start_sum, '秒')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 验证正向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#第二层的输出\n",
    "z2=np.dot(X_test,w1)\n",
    "a2=sigmoid(z2)\n",
    "    \n",
    "#第三层输出\n",
    "z3=np.dot(a2,w2)\n",
    "a3=sigmoid(z3)\n",
    "\n",
    "#第四层输出\n",
    "z4=np.dot(a3,w3)\n",
    "a4=sigmoid(z4)\n",
    "        \n",
    "#反向传播   \n",
    "#cost = (abs(y_train - a4)).mean()\n",
    "cost = 0.5 * (y_test - a4)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 将输出变为单一向量形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.zeros((a4.shape[0],a4.shape[1]))\n",
    "for i in range(a4.shape[0]):\n",
    "    for j in range(a4.shape[1]):\n",
    "        if LA.norm(a4[i],np.inf)==a4[i,j]:\n",
    "            arr[i,j]=1\n",
    "        else:\n",
    "            arr[i,j]=0\n",
    "            \n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 计算准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.322\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.65      0.37        43\n",
      "           1       0.43      0.87      0.58        61\n",
      "           2       0.26      0.18      0.21        62\n",
      "           3       0.20      0.11      0.15        35\n",
      "           4       0.12      0.04      0.06        49\n",
      "           5       0.29      0.43      0.34        49\n",
      "           6       0.40      0.65      0.50        55\n",
      "           7       0.20      0.06      0.09        49\n",
      "           8       0.00      0.00      0.00        48\n",
      "           9       0.50      0.06      0.11        49\n",
      "\n",
      "   micro avg       0.32      0.32      0.32       500\n",
      "   macro avg       0.27      0.31      0.24       500\n",
      "weighted avg       0.27      0.32      0.25       500\n",
      " samples avg       0.32      0.32      0.32       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,arr))\n",
    "print(classification_report(y_test,arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4 可视化训练集上的MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9ed0c622e8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(5,3))\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.plot(range(0,300),lis,label='MSE in train')\n",
    "ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验小结\n",
    "\n",
    "在本实验中，你对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。讨论了分段计算在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其『链』起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。\n",
    "\n",
    "最后，建议观看 3Blue1Brown 制作的 [深度学习之反向传播算法视频](https://www.bilibili.com/video/BV16x411V7Qg?p=1)，获得更直观的理解。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
